{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import rdkit.Chem as Chem\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\28317\\AppData\\Local\\Temp\\ipykernel_52580\\557473034.py:8: DtypeWarning: Columns (1,2,3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dt = pd.read_csv(path).iloc[:, :3]\n"
     ]
    }
   ],
   "source": [
    "filePath = '../xtb_ml_data' # 提取全部的data，以及sensitizer和emitter\n",
    "xtb_data = os.listdir(filePath)\n",
    "dt_list = []\n",
    "\n",
    "j = 0\n",
    "for i in xtb_data:\n",
    "    path = filePath + \"/\" + i\n",
    "    dt = pd.read_csv(path).iloc[:, :3]\n",
    "    dt.columns = ['SMILES', 'xTB_S1', 'xTB_T1']\n",
    "    dt = dt[~dt['xTB_S1'].isin(['Invalid SMILES'])]\n",
    "    dt['T1S1ratio'] = pd.to_numeric(dt['xTB_T1']) / pd.to_numeric(dt['xTB_S1'])\n",
    "    dt_list.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全部数据合并\n",
    "dt_tot = pd.concat(dt_list)  \n",
    "dt_tot = dt_tot.drop_duplicates(subset = 'SMILES', keep=False)\n",
    "dt_tot.dropna(inplace = True)\n",
    "dt_tot.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606590/606590 [01:46<00:00, 5681.57it/s] \n"
     ]
    }
   ],
   "source": [
    "# 去掉全部带电的分子\n",
    "index = []\n",
    "for i in tqdm(range(len(dt_tot))):\n",
    "    mol = Chem.MolFromSmiles(dt_tot.iloc[i, 0])\n",
    "    if mol == None: \n",
    "        continue\n",
    "    Chem.Kekulize(mol)\n",
    "    if abs(Chem.GetFormalCharge(mol)) == 0:\n",
    "        index.append(i)\n",
    "\n",
    "dt_tot = dt_tot.iloc[index, :]\n",
    "dt_tot.reset_index(drop = True, inplace = True)\n",
    "dt_emit = dt_tot[(dt_tot['T1S1ratio'] > (1/2.2)) & (dt_tot['T1S1ratio'] < (1/1.8))]\n",
    "dt_sens = dt_tot[(dt_tot['T1S1ratio'] > 0.8) & (dt_tot['T1S1ratio'] < 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分测试集\n",
    "np.random.seed(2022)\n",
    "def is_large(smi): # split the target data according to atom_num > 20\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    atoms_num = mol.GetNumAtoms()\n",
    "    if atoms_num > 20:\n",
    "        return True\n",
    "    if atoms_num <= 20:\n",
    "        return False\n",
    "\n",
    "test_rand = dt_tot.sample(n = 3000, replace = False)\n",
    "test_emit = dt_emit.sample(n = 3000, replace = False)\n",
    "test_sens = dt_sens.sample(n = 3000, replace = False)\n",
    "test_tot = pd.concat([test_rand, test_emit, test_sens])  \n",
    "test_tot = test_tot.drop_duplicates(subset = 'SMILES', keep=False)\n",
    "test_tot['is_large'] = test_tot['SMILES'].apply(lambda x: is_large(x))\n",
    "test_large = test_tot[test_tot['is_large'] == True]\n",
    "test_small = test_tot[test_tot['is_large'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rand.to_csv('../test_set/test_rand.csv')\n",
    "test_emit.to_csv('../test_set/test_emit.csv')\n",
    "test_sens.to_csv('../test_set/test_sens.csv')\n",
    "test_large.to_csv('../test_set/test_large.csv')\n",
    "test_small.to_csv('../test_set/test_small.csv')\n",
    "test_tot.to_csv('../test_set/test_tot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tot_dup = pd.concat([test_tot.iloc[:, :-1], dt_tot])\n",
    "dt_tot = dt_tot_dup.drop_duplicates(subset = 'SMILES', keep=False)\n",
    "dt_tot.reset_index(drop = True, inplace = True)\n",
    "dt_train = dt_tot.sample(n = 20000, replace = False)\n",
    "dt_train.to_csv('init_train.csv') # 第一轮训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tot_round1 = pd.concat([dt_train, dt_tot])\n",
    "dt_tot_round1 = dt_tot_round1.drop_duplicates(subset = 'SMILES', keep = False)\n",
    "dt_tot_round1.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_tot_round1.to_csv('dt_tot_round1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_and_pred(dt_tot_csv, cut_len, round, check_point_path):\n",
    "    import math\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    os.mkdir(f'../cut_data/origin_cut/round_{round}')\n",
    "    os.mkdir(f'../cut_data/pred_cut/round_{round}')\n",
    "\n",
    "    dt_tot = pd.read_csv(dt_tot_csv)\n",
    "    dt_tot.reset_index(drop = True, inplace = True)\n",
    "    tot_len = len(dt_tot)\n",
    "    file_num = math.ceil(tot_len / cut_len)\n",
    "    for i in range(file_num):\n",
    "        if i != (file_num - 1):\n",
    "            dt_tot.iloc[i*cut_len:(i+1)*cut_len, :].to_csv(f'../cut_data/origin_cut/round_{round}/dt_tot_{i}.csv')\n",
    "        if i == (file_num - 1):\n",
    "            dt_tot.iloc[i*cut_len:, :].to_csv(f'../cut_data/origin_cut/round_{round}/dt_tot_{i}.csv')\n",
    "    for i in range(file_num):\n",
    "        #print(i)\n",
    "        test_path = f'../cut_data/origin_cut/round_{round}/dt_tot_{i}.csv'\n",
    "        pred_path = f'../cut_data/pred_cut/round_{round}/dt_tot_{i}.csv'\n",
    "        os.system(f'chemprop_predict --test_path {test_path} --checkpoint_dir {check_point_path} --preds_path {pred_path} --smiles_column SMILES --ensemble_variance --num_workers 0')\n",
    "    \n",
    "    dt_list = []\n",
    "    for i in range(file_num):\n",
    "        dt_path = f'../cut_data/pred_cut/round_{round}/dt_tot_{i}.csv'\n",
    "        dt = pd.read_csv(dt_path)\n",
    "        dt_list.append(dt)\n",
    "    pred_tot = pd.concat(dt_list).dropna()\n",
    "    pred_tot.reset_index(drop = True, inplace = True)\n",
    "    pred_tot = pred_tot[~pred_tot['xTB_S1'].isin(['Invalid SMILES'])]\n",
    "    pred_tot['uncertainty_tot'] = pred_tot['xTB_S1_epi_unc'].apply(lambda x: float(x)) + pred_tot['xTB_T1_epi_unc'].apply(lambda x: float(x))\n",
    "    pred_tot.sort_values(by = 'uncertainty_tot', ascending = False, inplace = True)\n",
    "    pred_index = pred_tot.index[:20000]\n",
    "    \n",
    "    newtrain = dt_tot.iloc[pred_index, :]\n",
    "    train_set = pd.read_csv(f'../train_set/train_round{round}.csv')\n",
    "    train_set = pd.concat([newtrain, train_set])\n",
    "    train_set.to_csv(f'../train_set/train_round{round+1}.csv')\n",
    "\n",
    "    dt_tot_new = dt_tot.iloc[list(set(pred_tot.index)-set(pred_index)), :]\n",
    "    dt_tot_new.reset_index(drop = True, inplace = True)\n",
    "    dt_tot_new.to_csv(f'../data_tot/dt_tot_round{round+1}.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do:\n",
    "- ~~把测试集输出成csv文件整理好~~\n",
    "- ~~修改cut_and_pred函数，把输入参数设定为csv，以及简单格式~~\n",
    "- ~~把后续的pred和uncertainty排序整合进原有的cut_and_pred中，使其返回csv格式文件~~\n",
    "- ~~编写好test脚本，计算针对不同test set的四个评价指标：MAE, RMSE, R^2, spearman correlation~~\n",
    "- 整理train代码，并封装至脚本中\n",
    "- 把train和cut_and_pred整合在一起\n",
    "- 把全流程整合在一起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_index(round, check_point_path):\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    filePath = '../test_set' \n",
    "    os.mkdir(f'../pred_set/round_{round}')\n",
    "    test_csv = os.listdir(filePath)\n",
    "    test_result_list = []\n",
    "\n",
    "    for i in test_csv:\n",
    "        test_path = filePath + '/' + i\n",
    "        test_set = pd.read_csv(test_path)\n",
    "        pred_path = f'../pred_set/round_{round}/{i}'\n",
    "        os.system(f'chemprop_predict --test_path {test_path} --checkpoint_dir {check_point_path} --preds_path {pred_path} --smiles_column SMILES --ensemble_variance --num_workers 0')\n",
    "        \n",
    "        pred_set = pd.read_csv(pred_path).dropna()\n",
    "        pred_set.reset_index(drop = True, inplace = True)\n",
    "        pred_set = pred_set[~pred_set['xTB_S1'].isin(['Invalid SMILES'])]\n",
    "        S1_uncertainty = pred_set['xTB_S1_epi_unc'].apply(lambda x: float(x))\n",
    "        T1_uncertainty = pred_set['xTB_T1_epi_unc'].apply(lambda x: float(x))\n",
    "\n",
    "        # calculate\n",
    "        S1_error = abs(test_set['xTB_S1'] - pred_set['xTB_S1'])\n",
    "        T1_error = abs(test_set['xTB_T1'] - pred_set['xTB_T1'])\n",
    "        S1_spearman_correlation = S1_uncertainty.corr(S1_error,'spearman')\n",
    "        T1_spearman_correlation = T1_uncertainty.corr(T1_error,'spearman')\n",
    "        S1_pearson_correlation = S1_uncertainty.corr(S1_error,'pearson')\n",
    "        T1_pearson_correlation = T1_uncertainty.corr(T1_error,'pearson')\n",
    "        S1_mae = S1_error.mean()\n",
    "        T1_mae = T1_error.mean()\n",
    "        S1_rmse = ((S1_error*S1_error).mean()) ** 0.5\n",
    "        T1_rmse = ((T1_error*T1_error).mean()) ** 0.5\n",
    "\n",
    "        # store the result in dictionary\n",
    "        test_dict = {'File_name' : i, 'S1_spearman_correlation' : S1_spearman_correlation, \n",
    "        'T1_spearman_correlation' : T1_spearman_correlation, 'S1_pearson_correlation' : S1_pearson_correlation,\n",
    "        'T1_pearson_correlation' : T1_pearson_correlation, 'S1_mae' : S1_mae, 'T1_mae' : T1_mae, 'S1_rmse' : S1_rmse, 'T1_rmse' : T1_rmse}\n",
    "        test_result_list.append(test_dict)\n",
    "\n",
    "    store_path = f'../test_performance/test_round{round}.csv'\n",
    "    pd.DataFrame(test_result_list).to_csv(store_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1_uncertainty = pred_set['xTB_S1_epi_unc'].apply(lambda x: float(x))\n",
    "T1_uncertainty = pred_set['xTB_T1_epi_unc'].apply(lambda x: float(x))\n",
    "\n",
    "# calculate mae\n",
    "S1_error = abs(test_set['xTB_S1'] - pred_set['xTB_S1'])\n",
    "T1_error = abs(test_set['xTB_T1'] - pred_set['xTB_T1'])\n",
    "spearman_correlation_S1 = S1_uncertainty.corr(S1_error,'spearman')\n",
    "spearman_correlation_T1 = T1_uncertainty.corr(T1_error,'spearman')\n",
    "pearson_correlation_S1 = S1_uncertainty.corr(S1_error,'pearson')\n",
    "pearson_correlation_T1 = T1_uncertainty.corr(T1_error,'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "round = 1\n",
    "filePath = '../test_set' \n",
    "test_csv = os.listdir(filePath)\n",
    "test_set_list = []\n",
    "i = test_csv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = filePath + '/' + i\n",
    "pred_path = f'../pred_set/round_{round}/{i}'\n",
    "check_point_path = '../model/round_1'\n",
    "os.system(f'chemprop_predict --test_path {test_path} --checkpoint_dir {check_point_path} --preds_path {pred_path} --smiles_column SMILES --ensemble_variance --num_workers 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_set = pd.read_csv(pred_path).dropna()\n",
    "#pred_set.reset_index(drop = True, inplace = True)\n",
    "#pred_set = pred_set[~pred_set['xTB_S1'].isin(['Invalid SMILES'])]\n",
    "pred_set['xTB_S1_epi_unc'] = pred_set['xTB_S1_epi_unc'].apply(lambda x: float(x))\n",
    "pred_set['xTB_T1_epi_unc'] = pred_set['xTB_T1_epi_unc'].apply(lambda x: float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = '../test_set' # 提取全部的data，以及sensitizer和emitter\n",
    "test_data = os.listdir(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95ead6bd2d45473fc34ca264613388bbb11a7ecfed696b93cdc96b68be756ec0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
